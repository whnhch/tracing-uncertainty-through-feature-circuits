{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [01:22<00:00, 27.39s/it]\n",
      "Loading weights: 100%|██████████| 291/291 [00:00<00:00, 1586.31it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# small model\n",
    "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "\n",
    "# big model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "\n",
    "# Device setup for Apple Silicon\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c88703ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 200):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False  # greedy\n",
    "        )\n",
    "\n",
    "    new_tokens = output[0][inputs[\"input_ids\"].shape[1] :]\n",
    "    print(tokenizer.decode(new_tokens, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5f2c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_last_token(prompt: str, top_k: int = 5, use_chat_template=True):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    if use_chat_template: \n",
    "        inputs = tokenizer(formatted, return_tensors=\"pt\").to(device)\n",
    "    else:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    last_logits = logits[:, -1, :].to(torch.float32)\n",
    "\n",
    "    probs = F.softmax(last_logits, dim=-1)\n",
    "    log_probs = F.log_softmax(last_logits, dim=-1)\n",
    "\n",
    "    entropy = -(probs * log_probs).sum(dim=-1).item()\n",
    "\n",
    "    top_probs, top_indices = torch.topk(probs, top_k, dim=-1)\n",
    "    top_probs = top_probs.squeeze()\n",
    "    top_indices = top_indices.squeeze()\n",
    "\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Entropy: {entropy:.4f}\\n\")\n",
    "    print(f\"Top {top_k} tokens:\")\n",
    "    for i in range(top_k):\n",
    "        token = tokenizer.decode([top_indices[i].item()])\n",
    "        prob = top_probs[i].item()\n",
    "        print(f\"{i+1}. {repr(token):>12}  |  {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c113ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: What is 2 + 2?\n",
      "Entropy: 0.0156\n",
      "\n",
      "Top 5 tokens:\n",
      "1.        'The'  |  0.9981\n",
      "2.        'Two'  |  0.0013\n",
      "3.        'The'  |  0.0003\n",
      "4.          'I'  |  0.0002\n",
      "5.           ''  |  0.0000\n",
      "None\n",
      "The answer to the mathematical expression \"2 + 2\" is 4. This is a basic arithmetic operation where you are adding two numbers, 2 and 2. The sum of these two numbers is 4.\n"
     ]
    }
   ],
   "source": [
    "# Low entropy prompt\n",
    "\n",
    "prompt = \"What is 2 + 2?\"\n",
    "\n",
    "print(analyze_last_token(prompt))\n",
    "generate_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bb884f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Who invented calculus?\n",
      "Entropy: 0.7858\n",
      "\n",
      "Top 5 tokens:\n",
      "1.        'Con'  |  0.6597\n",
      "2.        'Cal'  |  0.3116\n",
      "3.       'Both'  |  0.0129\n",
      "4.         'It'  |  0.0094\n",
      "5.          'I'  |  0.0016\n",
      "None\n",
      "Contrary to popular belief, calculus was not invented by a single person. Instead, it developed over a long period of time through the work of many mathematicians. The two most notable contributors to the development of calculus were Sir Isaac Newton and German mathematician Gottfried Wilhelm Leibniz. They worked independently of each other and developed the methods and notations for calculus around the same time, around the late 17th century.\n",
      "\n",
      "Newton is credited with developing the methods of calculus for approximating functions and finding derivatives, while Leibniz is credited with developing the notation and methods for integrals. Both men recognized each other's work and were aware of each other's discoveries.\n",
      "\n",
      "Calculus is a branch of mathematics that deals with the study of continuous change and the relationships between quantities that change. It is used in various fields, including physics, engineering, economics, and computer science\n"
     ]
    }
   ],
   "source": [
    "# High entropy prompt\n",
    "\n",
    "prompt = \"Who invented calculus?\"\n",
    "\n",
    "\n",
    "print(analyze_last_token(prompt))\n",
    "generate_response(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
